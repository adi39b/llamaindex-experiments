<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# Read the documentation for dspy for python LLM application. Assume I have a dspy module available and I want to asynchronously run it with several workers and record and save results of the run. Create the code specifically for this part

To asynchronously run a DSPy module with multiple workers and record and save the results, you can use DSPy's built-in `asyncify` function along with usage tracking and saving capabilities. Here's a complete implementation:

```python
import asyncio
import time
import dspy
import ujson
from datetime import datetime
import os

# Configure DSPy settings with your preferred language model and worker count
dspy.settings.configure(
    lm=dspy.LM("openai/gpt-4o-mini"),
    async_max_workers=4,  # Set your desired number of workers
    track_usage=True  # Enable LM usage tracking
)

# Assuming you already have a DSPy module defined like this:
# my_dspy_module = YourDSPyModule()

# Wrap your module with asyncify to support asynchronous execution
async_module = dspy.asyncify(my_dspy_module)

# Function to process a single input and save results
async def process_input(input_data, save_dir):
    start_time = time.time()
    
    # Execute the DSPy module asynchronously
    result = await async_module(question=input_data)
    
    # Get usage statistics
    usage_stats = result.get_lm_usage()
    
    # Prepare result data with metadata
    processed_result = {
        "input": input_data,
        "output": result.toDict(),
        "usage_stats": usage_stats,
        "processing_time": time.time() - start_time
    }
    
    # Save individual result to a unique JSON file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    filename = f"{save_dir}/result_{timestamp}.json"
    os.makedirs(save_dir, exist_ok=True)
    
    with open(filename, "w") as f:
        ujson.dump(processed_result, f, indent=2)
    
    return processed_result

# Function to process a batch of inputs concurrently
async def process_batch(inputs, save_dir="results", save_model=True):
    # Create tasks for each input
    tasks = [process_input(input_data, save_dir) for input_data in inputs]
    
    # Run all tasks concurrently and collect results
    results = await asyncio.gather(*tasks)
    
    # Aggregate results
    combined_results = {
        "timestamp": datetime.now().isoformat(),
        "total_inputs": len(inputs),
        "results": results
    }
    
    # Save combined results
    os.makedirs(save_dir, exist_ok=True)
    with open(f"{save_dir}/combined_results.json", "w") as f:
        ujson.dump(combined_results, f, indent=2)
    
    # Save the DSPy module if requested
    if save_model:
        model_dir = f"{save_dir}/model"
        os.makedirs(model_dir, exist_ok=True)
        my_dspy_module.save(model_dir, save_program=True)
        
        # Alternative: save state only to a JSON file
        # my_dspy_module.save(f"{save_dir}/model_state.json", save_program=False)
    
    return combined_results

# Example usage
async def main():
    # Sample list of inputs to process
    sample_inputs = [
        "What is the capital of France?",
        "Explain quantum computing in simple terms",
        "How does neural networking work?",
        "What are the benefits of clean energy?",
        "Describe the process of photosynthesis"
    ]
    
    # Process all inputs concurrently and save results
    results = await process_batch(sample_inputs)
    print(f"Processed {len(results['results'])} inputs")

# Run the async code
if __name__ == "__main__":
    asyncio.run(main())
```

This code:

1. Configures DSPy with your language model, sets the number of async workers (4 in this example), and enables usage tracking[^1_1][^1_7]
2. Wraps your DSPy module with `asyncify` to support asynchronous execution[^1_3][^1_7]
3. Creates functions to:
    - Process individual inputs and save detailed results
    - Handle batches of inputs concurrently using `asyncio.gather()`
4. Tracks and saves:
    - Input/output data
    - Language model usage statistics for each prediction[^1_1]
    - Processing time for each input
    - Timestamp information
5. Saves both individual results and combined results as JSON files[^1_6]
6. Optionally saves the entire DSPy module (architecture + state) for future use[^1_6]

You can adjust the number of workers based on your requirements and system capabilities. The default is 8, but you can configure it based on your workload and available resources[^1_7].

<div>‚ÅÇ</div>

[^1_1]: https://dspy.ai/learn/programming/modules/

[^1_2]: https://dspy.ai/learn/programming/language_models/

[^1_3]: https://dspy.ai/api/utils/asyncify/

[^1_4]: https://dspy.ai/api/modules/Parallel/

[^1_5]: https://dspy.ai/tutorials/rag/

[^1_6]: https://dspy.ai/tutorials/saving/

[^1_7]: https://dspy.ai/tutorials/deployment/

[^1_8]: https://mlflow.org/docs/latest/llms/dspy/index.html

[^1_9]: https://mlflow.org/docs/latest/python_api/mlflow.dspy.html

[^1_10]: https://www.datacamp.com/blog/dspy-introduction

[^1_11]: https://www.reddit.com/r/LocalLLaMA/comments/1cplfph/who_is_using_dspy/

[^1_12]: https://www.ibm.com/think/topics/dspy

[^1_13]: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/dspy-usecases.md

[^1_14]: https://github.com/stanfordnlp/dspy

[^1_15]: https://docs.together.ai/docs/dspy

[^1_16]: https://docs.notdiamond.ai/docs/dspy-tutorial

[^1_17]: https://github.com/dmatrix/genai-cookbook/blob/main/dspy/README.md

[^1_18]: https://docs.parea.ai/integrations/dspy/dspy

[^1_19]: https://github.com/stanfordnlp/dspy/issues/1812

[^1_20]: https://github.com/stanfordnlp/dsp/issues/54

[^1_21]: https://stackoverflow.com/questions/53214243/how-to-make-a-python-generator-execution-asynchronous

[^1_22]: https://www.youtube.com/watch?v=ELyk69DCiGQ

[^1_23]: https://github.com/stanfordnlp/dspy/issues/1126

[^1_24]: https://python.langchain.com/docs/integrations/providers/dspy/

[^1_25]: https://github.com/ALucek/dspy-breakdown/blob/main/dspy_breakdown.ipynb

[^1_26]: https://pypi.org/project/dspy/2.5.6/

[^1_27]: https://www.kaggle.com/code/marcinrutecki/stop-prompting-start-programming-dspy-examples

[^1_28]: https://github.com/stanfordnlp/dspy/issues/842

[^1_29]: https://github.com/stanfordnlp/dspy/issues/617

[^1_30]: https://developer.dataiku.com/latest/tutorials/genai/techniques-and-tools/auto-prompt/index.html

[^1_31]: https://dspy.ai/faqs/

[^1_32]: https://simmering.dev/blog/absa-with-dspy/

[^1_33]: https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/python_interpreter.py

[^1_34]: https://www.ibm.com/think/tutorials/prompt-engineering-with-dspy

[^1_35]: https://dev.to/ashokan/a-beginner-friendly-tutorial-using-dspy-to-enhance-prompt-engineering-with-openai-apis-1nbn

[^1_36]: https://dspy.ai

[^1_37]: https://www.youtube.com/watch?v=_ROckQHGHsU

[^1_38]: https://pyimagesearch.com/2024/09/09/llmops-with-dspy-build-rag-systems-using-declarative-programming/

[^1_39]: https://python.langchain.com/v0.1/docs/integrations/providers/dspy/

[^1_40]: https://stackoverflow.com/questions/64151548/how-to-add-multiple-workers-in-a-procfile

[^1_41]: https://dspy.ai/tutorials/multihop_search/

[^1_42]: https://dspy.ai/api/modules/Module/

[^1_43]: https://github.com/stanfordnlp/dspy/issues/1786

[^1_44]: https://dspy.ai/tutorials/observability/

---

# How can I configure the maximum number of parallel workers in the asyncio loop

There are several ways to control the maximum number of parallel workers in asyncio applications. Here are the most common approaches:

## Using asyncio.Semaphore

The most straightforward way to limit concurrency in asyncio is by using a semaphore:

```python
import asyncio

# Limit concurrency to 3 parallel tasks
limit = asyncio.Semaphore(3)

async def task_function(task_id):
    # This ensures only 3 tasks can execute this block simultaneously
    async with limit:
        print(f"Task {task_id} started")
        await asyncio.sleep(2)  # Simulating work
        print(f"Task {task_id} completed")
    
async def main():
    # Create 10 tasks, but only 3 will run in parallel
    tasks = [task_function(i) for i in range(10)]
    await asyncio.gather(*tasks)

asyncio.run(main())
```

This approach limits how many coroutines can execute a specific section of code simultaneously, not the total number in the event loop[^2_1][^2_3][^2_6].

## Implementing a Custom Worker Pool

For more control, you can implement a worker pool that processes tasks from a queue with a fixed number of workers:

```python
import asyncio

class AsyncWorkerPool:
    def __init__(self, num_workers):
        self.queue = asyncio.Queue()
        self.num_workers = num_workers
        self.workers = []
    
    async def worker(self):
        while True:
            task_func, args, kwargs = await self.queue.get()
            try:
                await task_func(*args, **kwargs)
            except Exception as e:
                print(f"Task error: {e}")
            finally:
                self.queue.task_done()
    
    async def start(self):
        self.workers = [asyncio.create_task(self.worker()) 
                        for _ in range(self.num_workers)]
    
    async def submit(self, task_func, *args, **kwargs):
        await self.queue.put((task_func, args, kwargs))
    
    async def join(self):
        await self.queue.join()
        for worker in self.workers:
            worker.cancel()
```

Usage example:

```python
async def main():
    pool = AsyncWorkerPool(num_workers=3)
    await pool.start()
    
    # Submit 10 tasks to the pool
    for i in range(10):
        await pool.submit(task_function, i)
    
    await pool.join()

asyncio.run(main())
```

This approach gives you more control over worker lifecycle and error handling[^2_4].

## Managing Task Batches with Semaphore

For processing larger batches of tasks while controlling concurrency:

```python
async def process_batch(items, concurrency_limit=5):
    semaphore = asyncio.Semaphore(concurrency_limit)
    
    async def process_with_limit(item):
        async with semaphore:
            # Add a delay if hitting the concurrency limit
            if semaphore.locked():
                print("Concurrency limit reached, waiting...")
                await asyncio.sleep(0.1)
            return await process_item(item)
    
    return await asyncio.gather(*[process_with_limit(item) for item in items])
```

This allows you to process a large number of items while ensuring only a specified number run concurrently[^2_1][^2_6].

## Using Third-Party Libraries

Some libraries provide built-in concurrency limiting:

```python
from aiostream import stream

async def main():
    # Process with at most 5 parallel tasks
    xs = range(100)
    ys = stream.map(process_item, xs, task_limit=5)
    results = await stream.list(ys)
```

This is a more elegant solution if you're already using or willing to add these libraries[^2_6].

Remember that asyncio itself doesn't have a direct configuration to limit the total number of coroutines in the event loop. Instead, you must implement these patterns to control concurrency at the application level.

<div>‚ÅÇ</div>

[^2_1]: http://rednafi.com/python/limit_concurrency_with_semaphore/

[^2_2]: https://learn.microsoft.com/en-us/azure/azure-functions/python-scale-performance-reference

[^2_3]: http://dobken.nl/posts/llm-async-tasks/

[^2_4]: https://gist.github.com/thehesiod/7081ab165b9a0d4de2e07d321cc2391d?permalink_comment_id=2611547

[^2_5]: https://superfastpython.com/threadpoolexecutor-number-of-threads/

[^2_6]: https://stackoverflow.com/questions/50308812/is-it-possible-to-limit-the-number-of-coroutines-running-corcurrently-in-asyncio/50309198

[^2_7]: https://stackoverflow.com/questions/68166306/correct-way-to-parallelize-work-with-asyncio

[^2_8]: https://docs.python.org/3/library/asyncio-eventloop.html

[^2_9]: https://stackoverflow.com/questions/48483348/how-to-limit-concurrency-with-python-asyncio

[^2_10]: https://superfastpython.com/asyncio-gather-limit-concurrency/

[^2_11]: https://testdriven.io/blog/concurrency-parallelism-asyncio/

[^2_12]: https://docs.python.org/3/library/asyncio-queue.html

[^2_13]: https://www.reddit.com/r/learnprogramming/comments/v0x1ka/is_there_a_limit_to_parallel_asynchronous/

[^2_14]: https://death.andgravity.com/limit-concurrency

[^2_15]: https://discuss.python.org/t/picking-the-number-of-workers-in-asyncio/7639

[^2_16]: https://github.com/python/cpython/issues/115201

[^2_17]: https://docs.python.org/3/library/asyncio-task.html

[^2_18]: https://docs.python.org/3/library/concurrent.futures.html

[^2_19]: https://superfastpython.com/processpoolexecutor-number-of-workers/

[^2_20]: https://discuss.python.org/t/adding-a-limit-parameter-to-taskgroup-for-concurrent-coroutines-management/47686

---

# Assume there is a dash application running to show the status of the runs. Create a page that connects to the previous asyncio process and displays the status in a dash table on screen

To create a Dash application that connects to an asyncio process and displays run status in a DataTable, we'll build a dashboard that monitors the results directory where your DSPy asyncio process saves its output. This approach allows the Dash app to run independently while still providing real-time updates.

```python
import dash
from dash import html, dcc, dash_table, Input, Output, callback
import pandas as pd
import json
import os
import glob
from datetime import datetime

# Initialize the Dash app
app = dash.Dash(__name__, title="DSPy Async Monitor")

# Define the path where the asyncio process saves results
RESULTS_DIR = "results"  # This should match the directory in your DSPy asyncio process

# Layout of the Dash app
app.layout = html.Div([
    html.H1("DSPy Async Process Monitor", style={'textAlign': 'center', 'marginBottom': '20px'}),
    
    # Interval component for refreshing the data automatically
    dcc.Interval(
        id='interval-component',
        interval=1000,  # refresh every second (1000ms)
        n_intervals=0
    ),
    
    # Status table
    dash_table.DataTable(
        id='status-table',
        columns=[
            {'name': 'Input', 'id': 'input'},
            {'name': 'Status', 'id': 'status'},
            {'name': 'Processing Time (s)', 'id': 'processing_time'},
            {'name': 'Token Usage', 'id': 'token_usage'},
            {'name': 'Last Updated', 'id': 'last_updated'}
        ],
        page_size=10,
        style_table={'overflowX': 'auto'},
        style_cell={
            'textAlign': 'left',
            'padding': '10px'
        },
        style_header={
            'backgroundColor': 'rgb(230, 230, 230)',
            'fontWeight': 'bold'
        },
        style_data_conditional=[
            {
                'if': {'filter_query': '{status} = "Completed"'},
                'backgroundColor': 'rgb(220, 255, 220)',
                'color': 'green'
            },
            {
                'if': {'filter_query': '{status} = "Processing"'},
                'backgroundColor': 'rgb(255, 255, 220)',
                'color': 'orange'
            },
            {
                'if': {'filter_query': '{status} = "Error"'},
                'backgroundColor': 'rgb(255, 220, 220)',
                'color': 'red'
            }
        ]
    ),
    
    # Summary section
    html.Div(id='summary-section', className='summary-container')
])

# Function to read results from the asyncio process files
def read_asyncio_results():
    results = []
    
    # Ensure the results directory exists
    if not os.path.exists(RESULTS_DIR):
        return results
    
    # Get all result files
    result_files = glob.glob(f"{RESULTS_DIR}/result_*.json")
    
    for file_path in result_files:
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            # Extract information from result file
            input_data = data.get('input', 'Unknown')
            output = data.get('output', None)
            processing_time = data.get('processing_time', 0)
            usage_stats = data.get('usage_stats', {})
            
            # Determine status
            if output:
                status = "Completed"
            elif 'error' in data:
                status = "Error"
            else:
                status = "Processing"
            
            # Calculate token usage
            prompt_tokens = usage_stats.get('prompt_tokens', 0)
            completion_tokens = usage_stats.get('completion_tokens', 0)
            total_tokens = prompt_tokens + completion_tokens
            
            # Get file modification time as last update time
            last_modified = datetime.fromtimestamp(os.path.getmtime(file_path))
            
            # Add to results list
            results.append({
                'input': input_data[:50] + ('...' if len(input_data) &gt; 50 else ''),
                'status': status,
                'processing_time': round(processing_time, 2),
                'token_usage': f"{total_tokens} ({prompt_tokens}+{completion_tokens})",
                'last_updated': last_modified.strftime("%Y-%m-%d %H:%M:%S")
            })
            
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
    
    return results

# Callback to update the DataTable and summary
@callback(
    [Output('status-table', 'data'),
     Output('summary-section', 'children')],
    Input('interval-component', 'n_intervals')
)
def update_status_table(n_intervals):
    # Read results from the asyncio process
    results = read_asyncio_results()
    
    # Calculate statistics
    completed = sum(1 for r in results if r['status'] == 'Completed')
    processing = sum(1 for r in results if r['status'] == 'Processing')
    errors = sum(1 for r in results if r['status'] == 'Error')
    
    # Calculate average processing time for completed tasks
    completed_times = [r['processing_time'] for r in results if r['status'] == 'Completed']
    avg_time = sum(completed_times) / len(completed_times) if completed_times else 0
    
    # Create summary section
    summary_children = [
        html.H3("Summary", style={'textAlign': 'center'}),
        html.Div([
            html.Div([
                html.P("Total Tasks"),
                html.H2(f"{len(results)}")
            ], className='summary-box'),
            html.Div([
                html.P("Completed"),
                html.H2(f"{completed}")
            ], className='summary-box completed'),
            html.Div([
                html.P("Processing"),
                html.H2(f"{processing}")
            ], className='summary-box processing'),
            html.Div([
                html.P("Errors"),
                html.H2(f"{errors}")
            ], className='summary-box error'),
            html.Div([
                html.P("Avg. Processing Time"),
                html.H2(f"{round(avg_time, 2)}s")
            ], className='summary-box')
        ], style={'display': 'flex', 'justifyContent': 'space-between', 'flexWrap': 'wrap'})
    ]
    
    # Check if there's a combined results file with additional information
    combined_file = f"{RESULTS_DIR}/combined_results.json"
    if os.path.exists(combined_file):
        try:
            with open(combined_file, 'r') as f:
                combined_data = json.load(f)
            
            # Extract additional information
            start_time = combined_data.get('timestamp', '')
            total_inputs = combined_data.get('total_inputs', 0)
            
            # Add to summary
            summary_children.append(html.Div([
                html.P(f"Run started: {start_time}"),
                html.P(f"Total expected inputs: {total_inputs}")
            ], style={'marginTop': '20px', 'textAlign': 'center'}))
        
        except Exception as e:
            print(f"Error reading combined results: {e}")
    
    return results, summary_children

# Add CSS for styling
app.index_string = '''

&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;DSPy Async Process Monitor&lt;/title&gt;
        &lt;meta charset="utf-8"&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;
        {%css%}
        &lt;style&gt;
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }
            h1 {
                color: #2c3e50;
            }
            .summary-container {
                margin-top: 30px;
                padding: 20px;
                background-color: white;
                border-radius: 5px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            }
            .summary-box {
                flex: 1;
                min-width: 150px;
                margin: 10px;
                padding: 15px;
                text-align: center;
                background-color: #f9f9f9;
                border-radius: 5px;
                box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            }
            .summary-box.completed {
                background-color: #e6f7e6;
            }
            .summary-box.processing {
                background-color: #fff7e6;
            }
            .summary-box.error {
                background-color: #f7e6e6;
            }
            .summary-box p {
                margin: 0;
                color: #7f8c8d;
            }
            .summary-box h2 {
                margin: 10px 0 0 0;
                color: #2c3e50;
            }
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
        {%app_entry%}
        &lt;footer&gt;
            {%config%}
            {%scripts%}
            {%renderer%}
        &lt;/footer&gt;
    &lt;/body&gt;
&lt;/html&gt;
'''

# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)
```


## Key Features

This Dash application:

1. **Automatically refreshes data**: Uses `dcc.Interval` to poll for updates every second, showing near real-time status without requiring manual refresh
2. **Status visualization**: Displays task information in a DataTable with color-coded status indicators (green for completed tasks, yellow for processing, red for errors)
3. **Summary statistics**: Shows a summary of completed, processing, and error tasks, with average processing time
4. **Token usage tracking**: Displays token consumption for each DSPy task, useful for monitoring API costs
5. **Connection to asyncio process**: Connects to your previous asyncio DSPy process by monitoring the results directory where it saves output files

## How It Works

The application works by:

1. Reading the JSON files created by your DSPy asyncio process from the specified results directory
2. Parsing the data to extract task status, processing time, and token usage information
3. Calculating summary statistics based on the collected data
4. Refreshing the dashboard every second to show the latest status

This approach keeps the Dash application completely separate from your asyncio process while still providing up-to-date monitoring capabilities. The asyncio process can run independently, and the Dash app simply observes its output files.

To run this dashboard alongside your existing DSPy asyncio process, save this code in a separate file (e.g., `dash_monitor.py`) and run it with `python dash_monitor.py`.

<div>‚ÅÇ</div>

[^3_1]: https://stackoverflow.com/questions/57786520/how-to-run-dash-in-async-with-asyncio

[^3_2]: https://ploomber.io/blog/dash-live-updates/

[^3_3]: https://dash.plotly.com/datatable

[^3_4]: https://dash.plotly.com/datatable/interactivity

[^3_5]: https://community.plotly.com/t/updating-dash-table-in-realtime-with-async-tick-data/25035

[^3_6]: https://dash.plotly.com/tutorial

[^3_7]: https://www.anaconda.com/blog/real-time-dashboard-streaming-refreshing

[^3_8]: https://www.youtube.com/watch?v=zHPaF8NoeuA

[^3_9]: https://panel.holoviz.org/how_to/callbacks/async.html

[^3_10]: https://github.com/plotly/dash/issues/94

[^3_11]: https://dash.plotly.com/live-updates

[^3_12]: https://community.plotly.com/t/running-calculation-heavy-process-in-background/17400

[^3_13]: https://discourse.holoviz.org/t/no-data-when-trying-to-make-a-real-time-dash-application-with-holoviews/5112

[^3_14]: https://www.iambobur.com/post/visualize-real-time-data-with-python-dash-and-risingwave

[^3_15]: https://www.youtube.com/watch?v=b0xD_G-QrjA

[^3_16]: https://stackoverflow.com/questions/51593378/dash-python-plotly-live-update-table/51632634

[^3_17]: https://github.com/tcbegley/dash-rq-demo

[^3_18]: https://www.youtube.com/watch?v=-j6a347tWX8

[^3_19]: https://plotly.com/python/table/

[^3_20]: https://stackoverflow.com/questions/72844831/how-to-display-a-dash-table-inside-an-html-container

[^3_21]: https://campus.datacamp.com/courses/building-dashboards-with-dash-and-plotly/advanced-dash-apps?ex=8

[^3_22]: https://www.youtube.com/watch?v=twHtUFR7rtw

[^3_23]: https://discourse.julialang.org/t/trying-to-use-dash-datatable/51519

[^3_24]: https://community.plotly.com/t/dash-table-datatable-styling-examples/15594

[^3_25]: https://pypi.org/project/dash-table/

[^3_26]: https://dash-bootstrap-components.opensource.faculty.ai/docs/components/table/

[^3_27]: https://stackoverflow.com/questions/54622557/python-dash-loading-pandas-dataframes-into-data-table

[^3_28]: https://docs.python.org/3/library/asyncio-task.html

[^3_29]: https://stackoverflow.com/questions/74550915/pulling-real-time-data-and-update-in-streamlit-and-asyncio

[^3_30]: https://community.plotly.com/t/python-asyncio-and-dash/28203

[^3_31]: https://github.com/plotly/dash/issues/111

[^3_32]: https://reflex.dev/blog/2023-09-28-unlocking-new-workflows-with-background-tasks/

[^3_33]: https://www.youtube.com/watch?v=stgbYj1QqsA

[^3_34]: https://stackoverflow.com/questions/71608087/python-dash-datatable-input

[^3_35]: https://www.youtube.com/watch?v=USTqY4gH_VM

[^3_36]: https://www.youtube.com/watch?v=dueejcyrYh8

---

# Rewrite the code assuming that the results are stored in a sqlite DB.

Additionally, I also want to show a progress bar indicating the status of each run for each input, based on the state of the asyncio run going on. How to implement this?

To create a Dash application that connects to an asyncio process using SQLite for storage and displays progress bars for each run, I'll implement a two-part solution:

## Part 1: Setting up the SQLite Database and Asyncio Process

First, let's set up the DSPy asyncio process to store results in SQLite:

```python
import asyncio
import time
import dspy
import sqlite3
from datetime import datetime
import os

# Initialize SQLite database
def init_database(db_path="dspy_results.db"):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create tasks table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS tasks (
        task_id INTEGER PRIMARY KEY AUTOINCREMENT,
        input_data TEXT NOT NULL,
        status TEXT NOT NULL,
        progress FLOAT DEFAULT 0,
        start_time TIMESTAMP,
        end_time TIMESTAMP,
        processing_time FLOAT,
        prompt_tokens INTEGER DEFAULT 0,
        completion_tokens INTEGER DEFAULT 0,
        total_tokens INTEGER DEFAULT 0,
        output TEXT,
        error TEXT
    )
    ''')
    
    # Create batch_runs table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS batch_runs (
        run_id INTEGER PRIMARY KEY AUTOINCREMENT,
        timestamp TIMESTAMP NOT NULL,
        total_tasks INTEGER DEFAULT 0,
        completed_tasks INTEGER DEFAULT 0
    )
    ''')
    
    conn.commit()
    conn.close()
    return db_path

# Configure DSPy settings
dspy.settings.configure(
    lm=dspy.LM("openai/gpt-4o-mini"),
    async_max_workers=4,
    track_usage=True
)

# Assuming you have a DSPy module defined
# my_dspy_module = YourDSPyModule()
async_module = dspy.asyncify(my_dspy_module)

# Update progress in database
async def update_progress(db_path, task_id, progress, status=None, output=None, error=None):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    update_query = "UPDATE tasks SET progress = ?"
    params = [progress]
    
    if status:
        update_query += ", status = ?"
        params.append(status)
    
    if output is not None:
        update_query += ", output = ?"
        params.append(output)
    
    if error is not None:
        update_query += ", error = ?"
        params.append(error)
    
    update_query += " WHERE task_id = ?"
    params.append(task_id)
    
    cursor.execute(update_query, params)
    
    if status == "Completed" or status == "Error":
        # Update end time and processing time
        end_time = datetime.now().isoformat()
        cursor.execute(
            "SELECT start_time FROM tasks WHERE task_id = ?", (task_id,)
        )
        start_time_str = cursor.fetchone()[^4_0]
        start_time = datetime.fromisoformat(start_time_str)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        cursor.execute(
            "UPDATE tasks SET end_time = ?, processing_time = ? WHERE task_id = ?",
            (end_time, processing_time, task_id)
        )
        
        # Update batch run completed tasks count
        cursor.execute(
            "UPDATE batch_runs SET completed_tasks = completed_tasks + 1 WHERE run_id = (SELECT MAX(run_id) FROM batch_runs)"
        )
    
    conn.commit()
    conn.close()

# Process a single input
async def process_input(input_data, db_path, task_id):
    try:
        # Update status to Processing
        await update_progress(db_path, task_id, 0.1, "Processing")
        
        # Simulate stages of processing with progress updates
        await asyncio.sleep(0.5)
        await update_progress(db_path, task_id, 0.3)
        
        # Execute the DSPy module asynchronously
        result = await async_module(question=input_data)
        
        await update_progress(db_path, task_id, 0.7)
        
        # Get usage statistics
        usage_stats = result.get_lm_usage()
        prompt_tokens = usage_stats.get('prompt_tokens', 0)
        completion_tokens = usage_stats.get('completion_tokens', 0)
        
        # Update token usage in database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute(
            "UPDATE tasks SET prompt_tokens = ?, completion_tokens = ?, total_tokens = ? WHERE task_id = ?",
            (prompt_tokens, completion_tokens, prompt_tokens + completion_tokens, task_id)
        )
        conn.commit()
        conn.close()
        
        # Mark as completed
        await update_progress(db_path, task_id, 1.0, "Completed", str(result.toDict()))
        
        return result
        
    except Exception as e:
        # Record error
        error_message = str(e)
        await update_progress(db_path, task_id, 0, "Error", error=error_message)
        raise

# Process a batch of inputs concurrently
async def process_batch(inputs, db_path="dspy_results.db"):
    # Initialize database
    init_database(db_path)
    
    # Create a new batch run record
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    timestamp = datetime.now().isoformat()
    cursor.execute(
        "INSERT INTO batch_runs (timestamp, total_tasks) VALUES (?, ?)",
        (timestamp, len(inputs))
    )
    run_id = cursor.lastrowid
    
    # Insert task records for each input
    task_ids = []
    for input_data in inputs:
        cursor.execute(
            "INSERT INTO tasks (input_data, status, progress, start_time) VALUES (?, ?, ?, ?)",
            (input_data, "Pending", 0, timestamp)
        )
        task_ids.append(cursor.lastrowid)
    
    conn.commit()
    conn.close()
    
    # Create tasks for each input
    tasks = [process_input(inputs[i], db_path, task_ids[i]) for i in range(len(inputs))]
    
    # Run all tasks concurrently
    try:
        results = await asyncio.gather(*tasks)
        
        # Save the model if needed
        model_dir = "model"
        os.makedirs(model_dir, exist_ok=True)
        my_dspy_module.save(model_dir, save_program=True)
        
        return results
    except Exception as e:
        print(f"Error in batch processing: {e}")
        raise

# Example usage
async def main():
    sample_inputs = [
        "What is the capital of France?",
        "Explain quantum computing in simple terms",
        "How does neural networking work?",
        "What are the benefits of clean energy?",
        "Describe the process of photosynthesis"
    ]
    
    results = await process_batch(sample_inputs)
    print(f"Processed {len(results)} inputs")

if __name__ == "__main__":
    asyncio.run(main())
```


## Part 2: Dash Monitoring App with Progress Bars

Now, let's create the Dash application that monitors the SQLite database and displays progress bars for each run:

```python
import dash
from dash import html, dcc, dash_table, Input, Output, callback
import pandas as pd
import sqlite3
from datetime import datetime
import dash_bootstrap_components as dbc

# Path to the SQLite database
DB_PATH = "dspy_results.db"

# Initialize the Dash app with Bootstrap
app = dash.Dash(__name__, 
                external_stylesheets=[dbc.themes.BOOTSTRAP],
                title="DSPy Async Monitor")

# Function to read task data from SQLite
def get_tasks_data():
    try:
        conn = sqlite3.connect(DB_PATH)
        
        # Read tasks data
        tasks_df = pd.read_sql_query(
            '''
            SELECT 
                task_id, 
                input_data, 
                status, 
                progress, 
                start_time, 
                end_time, 
                processing_time,
                prompt_tokens,
                completion_tokens,
                total_tokens
            FROM tasks
            ORDER BY task_id
            ''', 
            conn
        )
        
        # Format data for display
        if not tasks_df.empty:
            # Format input data (truncate if too long)
            tasks_df['input_display'] = tasks_df['input_data'].apply(
                lambda x: x[:50] + ('...' if len(x) &gt; 50 else '')
            )
            
            # Format processing time
            tasks_df['processing_time'] = tasks_df['processing_time'].apply(
                lambda x: round(x, 2) if pd.notnull(x) else 0
            )
            
            # Format token usage
            tasks_df['token_usage'] = tasks_df.apply(
                lambda row: f"{row['total_tokens']} ({row['prompt_tokens']}+{row['completion_tokens']})" 
                if pd.notnull(row['total_tokens']) else "0",
                axis=1
            )
            
            # Format last updated time
            tasks_df['last_updated'] = tasks_df.apply(
                lambda row: datetime.fromisoformat(row['end_time']).strftime("%Y-%m-%d %H:%M:%S") 
                if pd.notnull(row['end_time']) 
                else datetime.fromisoformat(row['start_time']).strftime("%Y-%m-%d %H:%M:%S") 
                if pd.notnull(row['start_time']) 
                else "Unknown",
                axis=1
            )
        
        conn.close()
        return tasks_df
    
    except Exception as e:
        print(f"Error reading tasks data: {e}")
        return pd.DataFrame()

# Function to read batch run information
def get_batch_info():
    try:
        conn = sqlite3.connect(DB_PATH)
        
        # Read the latest batch run
        batch_df = pd.read_sql_query(
            '''
            SELECT 
                run_id, 
                timestamp, 
                total_tasks, 
                completed_tasks
            FROM batch_runs
            ORDER BY run_id DESC
            LIMIT 1
            ''', 
            conn
        )
        
        conn.close()
        
        if not batch_df.empty:
            return batch_df.iloc[^4_0].to_dict()
        else:
            return None
    
    except Exception as e:
        print(f"Error reading batch info: {e}")
        return None

# Layout of the Dash app
app.layout = html.Div([
    html.H1("DSPy Async Process Monitor", style={'textAlign': 'center', 'marginBottom': '20px'}),
    
    # Interval component for refreshing data automatically
    dcc.Interval(
        id='interval-component',
        interval=1000,  # refresh every second
        n_intervals=0
    ),
    
    # Summary statistics
    html.Div(id='status-cards', style={'marginBottom': '20px'}),
    
    # Task progress bars
    html.Div([
        html.H3("Task Progress", style={'textAlign': 'center', 'marginBottom': '10px'}),
        html.Div(id='tasks-progress')
    ], style={'marginBottom': '30px'}),
    
    # Detailed task table
    html.Div([
        html.H3("Task Details", style={'textAlign': 'center', 'marginBottom': '10px'}),
        dash_table.DataTable(
            id='status-table',
            columns=[
                {'name': 'ID', 'id': 'task_id'},
                {'name': 'Input', 'id': 'input_display'},
                {'name': 'Status', 'id': 'status'},
                {'name': 'Progress', 'id': 'progress'},
                {'name': 'Processing Time (s)', 'id': 'processing_time'},
                {'name': 'Token Usage', 'id': 'token_usage'},
                {'name': 'Last Updated', 'id': 'last_updated'}
            ],
            page_size=10,
            style_table={'overflowX': 'auto'},
            style_cell={
                'textAlign': 'left',
                'padding': '10px'
            },
            style_header={
                'backgroundColor': 'rgb(230, 230, 230)',
                'fontWeight': 'bold'
            },
            style_data_conditional=[
                {
                    'if': {'filter_query': '{status} = "Completed"'},
                    'backgroundColor': 'rgb(220, 255, 220)',
                    'color': 'green'
                },
                {
                    'if': {'filter_query': '{status} = "Processing"'},
                    'backgroundColor': 'rgb(255, 255, 220)',
                    'color': 'orange'
                },
                {
                    'if': {'filter_query': '{status} = "Error"'},
                    'backgroundColor': 'rgb(255, 220, 220)',
                    'color': 'red'
                },
                {
                    'if': {'filter_query': '{status} = "Pending"'},
                    'backgroundColor': 'rgb(240, 240, 240)',
                    'color': 'gray'
                }
            ]
        )
    ])
])

# Callback to update the status cards
@callback(
    Output('status-cards', 'children'),
    Input('interval-component', 'n_intervals')
)
def update_status_cards(n_intervals):
    batch_info = get_batch_info()
    
    if not batch_info:
        return html.Div(
            html.P("No batch runs found. Start the DSPy process to see statistics."),
            style={'textAlign': 'center', 'padding': '20px'}
        )
    
    # Get task statistics
    tasks_df = get_tasks_data()
    completed = sum(tasks_df['status'] == 'Completed')
    processing = sum(tasks_df['status'] == 'Processing')
    pending = sum(tasks_df['status'] == 'Pending')
    errors = sum(tasks_df['status'] == 'Error')
    
    # Calculate average processing time for completed tasks
    avg_time = 0
    completed_times = tasks_df[tasks_df['status'] == 'Completed']['processing_time']
    if len(completed_times) &gt; 0:
        avg_time = completed_times.mean()
    
    # Calculate overall progress percentage
    overall_progress = 0
    if batch_info['total_tasks'] &gt; 0:
        overall_progress = (batch_info['completed_tasks'] / batch_info['total_tasks']) * 100
    
    # Create status cards
    return dbc.Row([
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Total Tasks"),
                dbc.CardBody(html.H2(batch_info['total_tasks']))
            ], color="light", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Completed"),
                dbc.CardBody(html.H2(completed))
            ], color="success", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Processing"),
                dbc.CardBody(html.H2(processing))
            ], color="warning", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Pending"),
                dbc.CardBody(html.H2(pending))
            ], color="secondary", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Errors"),
                dbc.CardBody(html.H2(errors))
            ], color="danger", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Avg. Time (s)"),
                dbc.CardBody(html.H2(f"{round(avg_time, 2)}"))
            ], color="info", outline=True),
            width=2
        ),
        dbc.Col(
            dbc.Card([
                dbc.CardHeader("Overall Progress"),
                dbc.CardBody([
                    html.H2(f"{round(overall_progress)}%"),
                    dbc.Progress(value=overall_progress, style={"height": "20px"})
                ])
            ], color="primary", outline=True),
            width=12,
            style={"marginTop": "15px"}
        )
    ])

# Callback to update the task progress bars
@callback(
    Output('tasks-progress', 'children'),
    Input('interval-component', 'n_intervals')
)
def update_task_progress(n_intervals):
    tasks_df = get_tasks_data()
    
    if tasks_df.empty:
        return html.Div(
            html.P("No tasks found. Start the DSPy process to see task progress."),
            style={'textAlign': 'center', 'padding': '20px'}
        )
    
    # Create a progress bar for each task
    progress_bars = []
    
    for _, row in tasks_df.iterrows():
        # Calculate progress percentage (0 to 100)
        progress_value = int(row['progress'] * 100)
        
        # Determine color based on status
        color = "secondary"  # Default (Pending)
        if row['status'] == 'Completed':
            color = "success"
        elif row['status'] == 'Processing':
            color = "warning"
        elif row['status'] == 'Error':
            color = "danger"
        
        # Create a label for the progress bar
        label = f"Task {row['task_id']}: {row['input_display']}"
        
        # Create progress bar component
        progress_bar = dbc.Card([
            dbc.CardBody([
                html.Div(label, style={'marginBottom': '5px'}),
                dbc.Progress(
                    value=progress_value,
                    color=color,
                    striped=(row['status'] == 'Processing'),  # Striped effect for processing tasks
                    animated=(row['status'] == 'Processing'),  # Animated effect for processing tasks
                    style={"height": "20px"}
                ),
                html.Div(
                    f"{progress_value}% - {row['status']}",
                    style={'textAlign': 'right', 'marginTop': '5px', 'fontSize': '0.8rem'}
                )
            ])
        ], className="mb-3")
        
        progress_bars.append(progress_bar)
    
    return html.Div(progress_bars)

# Callback to update the status table
@callback(
    Output('status-table', 'data'),
    Input('interval-component', 'n_intervals')
)
def update_status_table(n_intervals):
    tasks_df = get_tasks_data()
    
    if tasks_df.empty:
        return []
    
    # Filter columns for display
    display_columns = [
        'task_id', 'input_display', 'status', 'progress', 
        'processing_time', 'token_usage', 'last_updated'
    ]
    
    # Convert progress to percentage for display
    tasks_df['progress'] = tasks_df['progress'].apply(
        lambda x: f"{int(x * 100)}%" if pd.notnull(x) else "0%"
    )
    
    return tasks_df[display_columns].to_dict('records')

# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)
```


## How This Solution Works

This implementation uses SQLite as the central database to connect the asyncio process with the Dash monitoring application:

1. **SQLite Database Structure**:
    - `tasks` table: Stores detailed information about each task, including input, status, progress, and results
    - `batch_runs` table: Stores information about each batch run, including total tasks and completed tasks
2. **Progress Tracking**:
    - Each task updates its progress in the database (0.0 to 1.0) as it moves through different stages
    - Status can be "Pending", "Processing", "Completed", or "Error"
    - The Dash app polls the database every second to get the latest progress information
3. **Progress Visualization**:
    - Individual progress bars for each task with color coding (green for completed, yellow for processing, red for errors)
    - Animated progress bars for tasks that are currently processing
    - Summary cards showing overall statistics
4. **Real-time Monitoring**:
    - The dashboard refreshes automatically every second
    - Detailed information table shows processing time, token usage, and status for each task

This solution keeps the asyncio process and Dash application completely separate, with SQLite serving as the intermediary data store. This architecture makes the system more robust and allows each component to run independently[^4_1][^4_2][^4_3].

<div>‚ÅÇ</div>

[^4_1]: https://github.com/plotly/dash-recipes/blob/master/dash_sqlite.py

[^4_2]: https://sqldocs.org/sqlite-database/flask-sqlite/

[^4_3]: https://dash.plotly.com/datatable

[^4_4]: https://community.plotly.com/t/custom-progress-information-on-each-loop-iteration-in-app-callback/33303

[^4_5]: https://stackoverflow.com/questions/76458992/how-to-implement-a-progress-bar-on-dash-app-based-on-callback-steps-progress

[^4_6]: https://codescandy.com/dashui/docs/progress.html

[^4_7]: https://www.youtube.com/watch?v=t1bKNj021do

[^4_8]: https://www.youtube.com/watch?v=dueejcyrYh8

[^4_9]: https://github.com/plotly/dash-table/issues/684

[^4_10]: https://flask.palletsprojects.com/en/stable/tutorial/database/

[^4_11]: https://community.plotly.com/t/i-need-to-create-a-editable-data-table-using-sql-db/66213

[^4_12]: https://www.youtube.com/watch?v=zHPaF8NoeuA

[^4_13]: https://community.plotly.com/t/connecting-database-to-flask-dash-app-using-flask-sqlalchemy-best-practices/76456

[^4_14]: https://dash.plotly.com/dash-enterprise/database-connections

[^4_15]: https://stackoverflow.com/questions/74762322/integrating-dash-and-flask-by-inserting-dash-chart-into-div-block-of-flask-templ

[^4_16]: https://www.youtube.com/watch?v=DWqEVpOfYxE

[^4_17]: https://community.plotly.com/t/sqlite-in-multi-page-dash-app/71879

[^4_18]: https://github.com/athioak7/carly

[^4_19]: https://hackersandslackers.com/plotly-dash-with-flask/

[^4_20]: https://community.plotly.com/t/how-to-show-a-loading-state-in-a-dashtable/57340

[^4_21]: https://www.statcan.gc.ca/en/data-science/network/low-code-ui-plotly-dash

[^4_22]: https://stackoverflow.com/questions/64347564/python-dash-datatable-with-data-bars-conditional-formating

[^4_23]: https://support.siteapppro.com/article/193-monitoring-progress-using-the-dashboard

[^4_24]: https://dash-bootstrap-components.opensource.faculty.ai/docs/components/progress/

[^4_25]: https://github.com/plotly/dash/issues/1914

[^4_26]: https://www.youtube.com/watch?v=b0xD_G-QrjA

[^4_27]: https://community.plotly.com/t/show-a-progress-bar-in-a-modal/78620

[^4_28]: https://www.thiscodeworks.com/dash-datatable-column-data-type-and-formatting-currency-decimals-groups-etc-python/632899191edec40015b48ab3

[^4_29]: https://dash.plotly.com/dash-html-components/progress

[^4_30]: https://campus.datacamp.com/courses/building-dashboards-with-dash-and-plotly/advanced-dash-apps?ex=8

[^4_31]: https://www.dash-mantine-components.com/components/progress

[^4_32]: https://www.reddit.com/r/learnpython/comments/nmj57x/dash_plotly_how_to_export_edited_data_from_data/

[^4_33]: https://www.reddit.com/r/learnpython/comments/9b9i1y/help_how_to_use_data_from_sql_in_plotly_dash/

[^4_34]: https://www.digitalocean.com/community/tutorials/how-to-use-an-sqlite-database-in-a-flask-application

[^4_35]: https://community.plotly.com/t/i-am-trying-to-connect-my-sqlite-database-table-to-dash/46644

[^4_36]: https://flask.palletsprojects.com/en/stable/patterns/sqlite3/

[^4_37]: https://stackoverflow.com/questions/68532879/access-sqlite-database-from-different-callbacks-in-plotly-dash-python

[^4_38]: https://dashlite.net/demo3/components/elements/progress.html

[^4_39]: https://community.atlassian.com/t5/Jira-questions/Task-Progress-displaying/qaq-p/2672125

